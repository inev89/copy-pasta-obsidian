Amazon's **S**imple **S**torage **S**ervice (S3)

# Find Region

 ## With Ping
 
```
ping flaws.cloud
PING flaws.cloud (52.218.180.2) 56(84) bytes of data.
64 bytes from s3-website-us-west-2.amazonaws.com (52.218.180.2): icmp_seq=1 ttl=128 time=48.3 ms
```

## With Curl

```
curl -I http://dev.huge-logistics.com
```

# Bucket Listings

On AWS you can set up S3 buckets with all sorts of permissions and functionality including using them to host static files. A number of people accidentally open them up with permissions that are too loose. Just like how you shouldn't allow directory listings of web servers, you shouldn't allow bucket listings.

#### Examples of this problem

- Directory listing of S3 bucket of Legal Robot ([link](https://hackerone.com/reports/163476)) and Shopify ([link](https://hackerone.com/reports/57505)).
- Read and write permissions to S3 bucket for Shopify again ([link](https://hackerone.com/reports/111643)) and Udemy ([link](https://hackerone.com/reports/131468)). This challenge did not have read and write permissions, as that would destroy the challenge for other players, but it is a common problem.

### Avoiding the mistake

By default, S3 buckets are private and secure when they are created. To allow it to be accessed as a web page, I had turn on "Static Website Hosting" and changed the bucket policy to allow everyone "s3:GetObject" privileges, which is fine if you plan to publicly host the bucket as a web page. But then to introduce the flaw, I changed the permissions to add "Everyone" to have "List" permissions. ![](http://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud/everyone.png) "Everyone" means everyone on the Internet. You can also list the files simply by going to [http://flaws.cloud.s3.amazonaws.com/](http://flaws.cloud.s3.amazonaws.com/) due to that List permission.

## Unauthenticated

```aws
aws s3 ls  s3://flaws.cloud/ --no-sign-request --region us-west-2
```


### Cyberduck

GUI based tool to look through cloud storages

```
https://cyberduck.io/?l=en
```

## With An Authenticated Account

```
aws s3 --profile YOUR_ACCOUNT ls s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud
```

## View Files in Bucket

### 1. **Download a Specific File**:

You can use the `aws s3 cp` command to download any of the listed files.

For example, to download `authenticated_users.png`:

```
aws s3 cp s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/authenticated_users.png .
```
### 2. **Download All Files in the Bucket**:

If you want to download all the files from that S3 path, use the `--recursive` flag:

```
aws s3 cp s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/ ./ --recursive
```


### 3. **View HTML Files Directly (Without Downloading)**:

You can view the content of small text-based files like `.html` and `.txt` directly from the S3 bucket using the `aws s3 cp` command with the `--no-sign-request` flag (if the bucket is public):

```
aws s3 cp s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/hint1.html -
```

### 4. **Download entire bucket with sync 

```
aws s3 sync s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/ . --no-sign-request --region us-west-2
```

# Find Owner of S3 Bucket

Create a user, policy and role to query for s3 buckets. [[3. Cloud/AWS/Misc|Create Offensive Infrastructure]]

```
python s3-account-search.py arn:aws:iam::427648302155:role/LeakyBucket s3://mega-big-tech --profile lab2-pwnedlabs
```

